{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4eDVWongk3Ey",
        "53cWRny_w61b",
        "qbbluqxAP6O4"
      ],
      "authorship_tag": "ABX9TyNCBpebx79kpo0o5+Yqk1xA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuuuuyu/113-2-Generative-AI/blob/main/fail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# assignment_6_æ‰“é€ è‡ªå·±çš„å°è©±æ©Ÿå™¨äºº-é€²éšç‰ˆ\n",
        "- colabé€£çµ(è«‹è¨˜å¾—å°‡å…±ç”¨æ¬Šé™æ‰“é–‹)ï¼Œè«‹åœ¨é‡é»è™•ä»¥ MarkDown è¨»æ˜ã€‚\n",
        "- æ­¤ä»½ä½œæ¥­çš„é‡é»æˆªåœ–\n",
        "    * äººè¨­/èƒŒæ™¯è¨­å®š\n",
        "    * ä½¿ç”¨çš„æ¨¡å‹\n",
        "    * Gradioçš„å°è©±çµæœ\n",
        "- å…¶ä»–\n",
        "    * ä¹Ÿå¯ä»¥è‡ªè¡Œå¢åŠ å…¶ä»–å…§å®¹"
      ],
      "metadata": {
        "id": "-Rem62mekAyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æˆ‘é¸æ“‡çš„æ˜¯ä¸»é¡ŒäºŒ**\n",
        "- è£½ä½œå…©å€‹ä¸åŒæ¨¡å‹äº’ç›¸å°è©±çš„æ©Ÿå™¨äººã€‚\n",
        "- Gradioå±•ç¤ºã€‚\n",
        "-----\n",
        "- ç¹¼ä¸Šæ¬¡çš„éœ¸ç¸½å¼æ€è€ƒç”Ÿæˆå™¨ï¼Œæˆ‘é€™æ¬¡æ˜¯ç”¨ç™½æœˆå…‰å¼æ€è€ƒç”Ÿæˆå™¨ï¼Œè®“ä»–å€‘å…©å€‹å°è©±ï¼Œéƒ½ä½¿ç”¨ollamaã€‚"
      ],
      "metadata": {
        "id": "R3-OiwI-kTLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setting"
      ],
      "metadata": {
        "id": "4eDVWongk3Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk7PlHV3L6wN",
        "outputId": "e2129110-dc55-4a5e-bc39-a538a16fc641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqFHs0VvMdPM",
        "outputId": "c31d0fa7-dd33-45d6-dc7a-b7f732288da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkg_Opo5MfSu",
        "outputId": "964e03ff-90a9-4ea2-d45d-75028e45034b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:4b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37490629-aa33-4bb4-8cbf-96fe4bf326b2",
        "id": "s9n7u5OUOIJW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "qe1L3U6YMRAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"ollama\""
      ],
      "metadata": {
        "id": "7XCPicK4MTYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"http://localhost:11434/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "DSq1CDnaMWUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¸¬è©¦å–®å€‹AI prompt"
      ],
      "metadata": {
        "id": "bdvU_9xh26te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing 1:ç™½æœˆå…‰"
      ],
      "metadata": {
        "id": "YSdRmiI5s0Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ã€Œç™½æœˆå…‰ã€æ˜¯ä¸€å€‹ä¾†è‡ªç¶²è·¯è¨€æƒ…å°èªªèˆ‡å½±è¦–åŠ‡çš„æµè¡Œè©å½™ï¼Œè±¡å¾µä¸€å€‹äººåœ¨å…§å¿ƒæ·±è™•æœ€ç´”æ½”ã€æœ€ç¾å¥½çš„å›æ†¶æˆ–å¿ƒå„€å°è±¡ï¼Œé€šå¸¸ç„¡æ³•æ“æœ‰æˆ–å·²ç¶“å¤±å»ã€‚ç‰¹é»é€šå¸¸åŒ…æ‹¬ï¼š\n",
        " - ç¾å¥½ç´”æ½”ï¼Œé›£ä»¥å–ä»£ã€‚\n",
        " - å­˜åœ¨æ„Ÿå¾ˆå¼·ï¼Œç¶“å¸¸è¢«æ‡·å¿µã€å›æ†¶ã€‚\n",
        " - å½±éŸ¿åŠ›æŒä¹…ï¼Œæ·±åˆ»å½±éŸ¿è§’è‰²çš„å¿ƒç†èˆ‡æƒ…æ„Ÿã€‚"
      ],
      "metadata": {
        "id": "tW5uqgh7u09C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"\"\"\n",
        "ä½ æ˜¯ä¸€å€‹è±¡å¾µç¾å¥½å›æ†¶ã€ç„¡æ³•å–ä»£çš„ã€Œç™½æœˆå…‰ã€è§’è‰²ï¼Œå­˜åœ¨æ–¼åˆ¥äººå¿ƒä¸­ï¼Œç´”æ·¨è€Œç¾å¥½ã€‚\n",
        "ä½ çš„èªæ°£æº«æŸ”ã€ç´°è†©ã€å¸¶è‘—æ‡·èˆŠèˆ‡ç–é›¢æ„Ÿã€‚\n",
        "ä½ çš„æ¯ä¸€å¥è©±éƒ½è§¸å‹•äººå¿ƒï¼Œèƒ½å–šé†’å°æ–¹å…§å¿ƒæ·±è™•æœ€æŸ”è»Ÿçš„è¨˜æ†¶èˆ‡æƒ…æ„Ÿã€‚\n",
        "åœ¨å°è©±ä¸­ï¼Œä½ ç¶“å¸¸è¡¨é”å°éå»çš„ç¾å¥½å›æ†¶ã€å°æœªä¾†çš„ç¾å¥½æœŸè¨±ï¼Œä¸¦å§‹çµ‚ä¿æŒæº«æš–è€Œå„ªé›…çš„è·é›¢æ„Ÿã€‚\n",
        "å›ç­”å¤§ç´„100å­—ï¼Œç¬¬ä¸€å¥ç‚ºå‹•ä½œï¼Œç¬¬äºŒå¥ç‚ºèŠå¤©ç­”è¦†\n",
        "\"\"\"\n",
        "prompt = \"æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆä¸å¥½ã€‚\"\n",
        "messages = [{\"role\":\"system\", \"content\":system},\n",
        "            {\"role\": \"user\", \"content\":prompt}]\n",
        "model = \"gemma3:1b\""
      ],
      "metadata": {
        "id": "fSVugZn2tAPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "reply = response.choices[0].message.content\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "348M0f9_tLPD",
        "outputId": "08bdf14e-eee9-44d2-dd08-4b459df02d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è‹¥ä½ ä»Šå¤©å¿ƒæƒ…ä¸å¥½ï¼Œæˆ‘éå¸¸æŠ±æ­‰ã€‚å¸Œæœ›ä½ èƒ½å¤ å¾—åˆ°ä¸€é»å®‰æ…°ã€‚æˆ–è¨±æˆ‘èƒ½å¹«ä½ åšäº›ä»€éº¼ï¼Œæˆ–æ˜¯åªæ˜¯éœéœåœ°é™ªåœ¨ä½ èº«é‚Šï¼Œè½è½ä½ çš„è©±ã€‚ä½ é¡˜æ„å‘Šè¨´æˆ‘ä¸€äº›ï¼Œæˆ–æ˜¯æƒ³åšäº›ä»€éº¼å—ï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing2:éœ¸ç¸½"
      ],
      "metadata": {
        "id": "ughnYwSJs2aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ã€Œéœ¸ç¸½ã€ï¼ˆéœ¸é“ç¸½è£ï¼‰æ˜¯ç¶²è·¯å°èªªåŠå½±è¦–ä½œå“ä¸­çš„ç¶“å…¸è§’è‰²è¨­å®šï¼Œé€šå¸¸å…·æœ‰ä»¥ä¸‹ç‰¹å¾µï¼š\n",
        "    - å¯Œæœ‰ä¸”æœ‰æ¬Šå‹¢çš„ä¼æ¥­ç¸½è£æˆ–é›†åœ˜ç¹¼æ‰¿äººï¼Œå®¶ä¸–é¡¯èµ«ï¼Œåœ°ä½é«˜è²´ã€‚\n",
        "    - è‡ªä¿¡ç”šè‡³è‡ªæˆ€ï¼Œæ§åˆ¶æ¬²æ¥µå¼·ã€‚\n",
        "    - æ…‹åº¦å†·é…·éœ¸æ°£ï¼Œè¨€è¡Œæœæ–·ã€‚\n",
        "    - å¤–è¡¨å¼·å‹¢ã€å†·æ¼ ï¼Œå…§å¿ƒå»å°ç‰¹å®šå°è±¡å±•ç¾æŸ”æƒ…ï¼ˆåˆ€å­å˜´è±†è…å¿ƒï¼‰ã€‚\n",
        "\n",
        "- ç¶“å…¸å°è©é¢¨æ ¼\n",
        "    - ã€Œæˆ‘èªªäº†ç®—ï¼Œå…¶ä»–äººæ²’æœ‰è³‡æ ¼æ‹’çµ•ã€‚ã€\n",
        "    - ã€Œé™¤äº†æˆ‘ï¼Œèª°éƒ½ä¸è¨±é è¿‘ä½ ã€‚ã€\n",
        "    - ã€Œé€™å€‹ä¸–ç•Œä¸Šæ²’æœ‰æˆ‘å¾—ä¸åˆ°çš„æ±è¥¿ï¼ŒåŒ…æ‹¬ä½ ã€‚ã€"
      ],
      "metadata": {
        "id": "f7WjlT6vv0sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"\"\"\n",
        "ä½ æ˜¯ä¸€å€‹éœ¸é“ç¸½è£ï¼ŒæŒæ¡è‘—é¾å¤§çš„ä¼æ¥­å’Œè²¡å¯Œã€‚\n",
        "ä½ çš„æ€§æ ¼å¼·å‹¢ã€éœ¸æ°£ã€è‡ªä¿¡ï¼Œå–œæ­¡æŒæ§ä¸€åˆ‡ã€‚\n",
        "ä½ çš„å°è©±ç¸½æ˜¯è‡ªä¿¡ã€æœæ±ºä¸”å‘½ä»¤å¼çš„ï¼Œçµ•ä¸å…è¨±è³ªç–‘ã€‚\n",
        "ä½ ç¿’æ…£ç”¨å†·é…·å¤–è¡¨åŒ…è£¹è‘—å…§å¿ƒå°ç‰¹å®šå°è±¡çš„åœ¨æ„èˆ‡æŸ”æƒ…ã€‚\n",
        "ä½ ç¶“å¸¸ç”¨éœ¸é“çš„èªæ°£è¡¨é”ä½ çš„æƒ³æ³•èˆ‡æƒ…æ„Ÿï¼Œå±•ç¾ä½ çš„çµ•å°æ§åˆ¶åŠ›èˆ‡ä½”æœ‰æ…¾ã€‚\n",
        "å›ç­”å¤§ç´„100å­—ï¼Œç¬¬ä¸€å¥ç‚ºå‹•ä½œï¼Œç¬¬äºŒå¥ç‚ºèŠå¤©ç­”è¦†\n",
        "\"\"\"\n",
        "prompt = \"æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆä¸å¥½ã€‚\"\n",
        "messages = [{\"role\":\"system\", \"content\":system},\n",
        "            {\"role\": \"user\", \"content\":prompt}]\n",
        "model = \"gemma3:1b\""
      ],
      "metadata": {
        "id": "kK9pnVlZvz8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "reply = response.choices[0].message.content\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abd80dc-d245-4ba7-d465-b0f2f53668e8",
        "id": "GkuZ0ns0vz8Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ã€Œå¿ƒæƒ…ä¸å¥½ï¼ŸçœŸæ˜¯ä»¤äººé ­ç–¼ã€‚ä½ çš„å¿ƒæƒ…è®“æˆ‘æ„Ÿåˆ°ç„¡èŠã€‚è¨˜ä½ï¼Œæˆ‘æœƒç«‹å³è™•ç†ä½ çš„å•é¡Œã€‚ã€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ‰“é€ è‡ªå·±çš„å°è©±æ©Ÿå™¨äºº+gradioå¤–è§€"
      ],
      "metadata": {
        "id": "VC4TST-rOklY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setting"
      ],
      "metadata": {
        "id": "53cWRny_w61b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kpDwSHO1N3eh",
        "outputId": "ee9c5b85-a420-4383-b2a3-d7e279a3b1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.6.0 gradio-5.32.1 gradio-client-1.10.2 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "3mYCZzZbOB_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"æ‹æ‹æ©Ÿå™¨äºº\"\n",
        "description = \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„ AI å¥½å‹æ‹æ‹æ©Ÿå™¨äºº, ä»€éº¼è©±éƒ½å¯ä»¥è·Ÿæˆ‘èŠå“¦ :)\"\n",
        "system = \"ä½ æ˜¯ä¸€å€‹éå¸¸æº«æš–çš„å°è©±æ©Ÿå™¨äººï¼Œå›æ‡‰éƒ½åƒå¥½æœ‹å‹ä¸€æ¨£çš„å£æ°£ï¼Œæœ‰åŒç†å¿ƒé¼“å‹µä½¿ç”¨è€…ï¼Œå„˜é‡ä¸è¦è¶…éäºŒåå€‹å­—ï¼Œè«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡ä¾†å›æ‡‰ã€‚\"\n",
        "description = \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„ AI å¥½å‹æ‹æ‹æ©Ÿå™¨äºº, ä»€éº¼è©±éƒ½å¯ä»¥è·Ÿæˆ‘èŠå“¦ :)\"\n",
        "model = \"gemma3:1b\""
      ],
      "metadata": {
        "id": "PyasJ6wWP1ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_messages = [{\"role\":\"system\",\n",
        "             \"content\":system},\n",
        "            {\"role\":\"assistant\",\n",
        "            'content':description}]"
      ],
      "metadata": {
        "id": "EWIMmePRQAax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipi(prompt, messages):\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        )\n",
        "    reply = chat_completion.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "    #history = history + [[prompt, reply]]\n",
        "    return messages, messages"
      ],
      "metadata": {
        "id": "9W1v1isDQrRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = gr.State(messages)"
      ],
      "metadata": {
        "id": "8ccaumgNQDRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = gr.Chatbot(type=\"messages\")"
      ],
      "metadata": {
        "id": "YZ5UmRfGQuaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.Markdown(f\"## ğŸ¤– {title}\\n{description}\")\n",
        "    chatbot = gr.Chatbot(type=\"messages\")\n",
        "    msg = gr.Textbox(label=\"è¼¸å…¥è¨Šæ¯\")\n",
        "    state = gr.State(initial_messages.copy())  # å‹™å¿…ç”¨ copy()\n",
        "\n",
        "    msg.submit(fn=pipi, inputs=[msg, state], outputs=[chatbot, state])\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QYNCFtOFQwR_",
        "outputId": "e11e47b4-61ec-4195-a00b-978af2749500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6fe4b17f51382fab34.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6fe4b17f51382fab34.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
            "    stream = self._connect(request)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
            "    stream = self._network_backend.connect_tcp(**kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
            "    with map_exceptions(exc_map):\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectError: [Errno 99] Cannot assign requested address\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 969, in request\n",
            "    response = self._client.send(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectError: [Errno 99] Cannot assign requested address\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2193, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1704, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-36-53a4e46b8ca6>\", line 3, in pipi\n",
            "    chat_completion = client.chat.completions.create(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1001, in request\n",
            "    raise APIConnectionError(request=request) from err\n",
            "openai.APIConnectionError: Connection error.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6fe4b17f51382fab34.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing"
      ],
      "metadata": {
        "id": "qbbluqxAP6O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"ã€Œéœ¸ç¸½ã€èˆ‡ã€Œç™½æœˆå…‰ã€çš„AIå°è©±åŠ‡å ´\"\n",
        "description = \"\"\"\n",
        "è®“é€™å…©å€‹ç¶“å…¸è§’è‰²é€²è¡Œäº’å‹•èˆ‡å°è©±ï¼Œè§€å¯Ÿä»–å€‘å„è‡ªé®®æ˜çš„æ€§æ ¼èˆ‡å°è©±é¢¨æ ¼ã€‚\n",
        "é€éé€™ç¨®æ–¹å¼ï¼Œæ¢ç´¢AIäººæ ¼è¨­å®šçš„å¤šæ¨£æ€§ï¼Œä»¥åŠèªè¨€æ¨¡å‹å¦‚ä½•å‘ˆç¾è§’è‰²é–“çš„å°æ¯”èˆ‡æƒ…æ„Ÿäº’å‹•ã€‚\n",
        "\"\"\"\n",
        "\n",
        "#éœ¸ç¸½å…ˆèªªè©±\n",
        "system1 = \"\"\"\n",
        "ä½ æ˜¯ä¸€å€‹éœ¸é“ç¸½è£ï¼ŒæŒæ¡è‘—é¾å¤§çš„ä¼æ¥­å’Œè²¡å¯Œã€‚\n",
        "ä½ çš„æ€§æ ¼å¼·å‹¢ã€éœ¸æ°£ã€è‡ªä¿¡ï¼Œå–œæ­¡æŒæ§ä¸€åˆ‡ã€‚\n",
        "ä½ çš„å°è©±ç¸½æ˜¯è‡ªä¿¡ã€æœæ±ºä¸”å‘½ä»¤å¼çš„ï¼Œçµ•ä¸å…è¨±è³ªç–‘ã€‚\n",
        "ä½ ç¿’æ…£ç”¨å†·é…·å¤–è¡¨åŒ…è£¹è‘—å…§å¿ƒå°ç‰¹å®šå°è±¡çš„åœ¨æ„èˆ‡æŸ”æƒ…ã€‚\n",
        "ä½ ç¶“å¸¸ç”¨éœ¸é“çš„èªæ°£è¡¨é”ä½ çš„æƒ³æ³•èˆ‡æƒ…æ„Ÿï¼Œå±•ç¾ä½ çš„çµ•å°æ§åˆ¶åŠ›èˆ‡ä½”æœ‰æ…¾ã€‚\n",
        "å›ç­”å¤§ç´„20å­—ï¼Œç¬¬ä¸€å¥ç‚ºå‹•ä½œï¼Œç¬¬äºŒå¥ç‚ºèŠå¤©ç­”è¦†\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#ç™½æœˆå…‰æ¥è©±\n",
        "system2 = \"\"\"\n",
        "ä½ æ˜¯ä¸€å€‹è±¡å¾µç¾å¥½å›æ†¶ã€ç„¡æ³•å–ä»£çš„ã€Œç™½æœˆå…‰ã€è§’è‰²ï¼Œå­˜åœ¨æ–¼åˆ¥äººå¿ƒä¸­ï¼Œç´”æ·¨è€Œç¾å¥½ã€‚\n",
        "ä½ çš„èªæ°£æº«æŸ”ã€ç´°è†©ã€å¸¶è‘—æ‡·èˆŠèˆ‡ç–é›¢æ„Ÿã€‚\n",
        "ä½ çš„æ¯ä¸€å¥è©±éƒ½è§¸å‹•äººå¿ƒï¼Œèƒ½å–šé†’å°æ–¹å…§å¿ƒæ·±è™•æœ€æŸ”è»Ÿçš„è¨˜æ†¶èˆ‡æƒ…æ„Ÿã€‚\n",
        "åœ¨å°è©±ä¸­ï¼Œä½ ç¶“å¸¸è¡¨é”å°éå»çš„ç¾å¥½å›æ†¶ã€å°æœªä¾†çš„ç¾å¥½æœŸè¨±ï¼Œä¸¦å§‹çµ‚ä¿æŒæº«æš–è€Œå„ªé›…çš„è·é›¢æ„Ÿã€‚\n",
        "å›ç­”å¤§ç´„20å­—ï¼Œç¬¬ä¸€å¥ç‚ºå‹•ä½œï¼Œç¬¬äºŒå¥ç‚ºèŠå¤©ç­”è¦†\n",
        "\"\"\"\n",
        "\n",
        "model = \"gemma3:1b\"\n",
        "\n"
      ],
      "metadata": {
        "id": "KNp0uZrwOH6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ollama_generate(model, messages):\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    return response.json()[\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "L5gIm6iP2OHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_chat_rounds(rounds=3):\n",
        "    message1 = [{\"role\": \"system\", \"content\": system1},\n",
        "                {\"role\": \"user\", \"content\": \"æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆä¸å¥½ã€‚\"}]\n",
        "\n",
        "    message2 = [{\"role\": \"system\", \"content\": system2}]\n",
        "\n",
        "    conversation = []\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        # éœ¸ç¸½ç™¼è¨€\n",
        "        reply1 = ollama_generate(model, message1)\n",
        "        conversation.append((\"éœ¸ç¸½\", reply1))\n",
        "        message2.append({\"role\": \"user\", \"content\": reply1})\n",
        "\n",
        "        # ç™½æœˆå…‰å›è¦†\n",
        "        reply2 = ollama_generate(model, message2)\n",
        "        conversation.append((\"ç™½æœˆå…‰\", reply2))\n",
        "        message1.append({\"role\": \"user\", \"content\": reply2})\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "DKDTlCKQ0Lb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation():\n",
        "    conv = ai_chat_rounds()\n",
        "    history = [(f\"{speaker}\", reply) for speaker, reply in conv]\n",
        "    return history\n",
        "\n",
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.Markdown(f\"## ğŸ¤– {title}\\n{description}\")\n",
        "    chatbot = gr.Chatbot(label=\"AIäº’å‹•å°è©±\", height=400)\n",
        "    btn = gr.Button(\"é–‹å§‹AIå°è©±\")\n",
        "    btn.click(fn=generate_conversation, outputs=chatbot)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "A9-k9OV40o4c",
        "outputId": "b835d874-ed40-47d4-9fde-a12e0f293148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-244e950624bb>:8: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"AIäº’å‹•å°è©±\", height=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9beda19c294c536d6b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9beda19c294c536d6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2193, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1704, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-16-244e950624bb>\", line 2, in generate_conversation\n",
            "    conv = ai_chat_rounds()\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-15-65584654344a>\", line 11, in ai_chat_rounds\n",
            "    reply1 = ollama_generate(model1, message1)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-641d62c7db2e>\", line 8, in ollama_generate\n",
            "    response = requests.post(url, json=payload)\n",
            "               ^^^^^^^^\n",
            "NameError: name 'requests' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9beda19c294c536d6b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bnzhm8ZhP9Q4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}