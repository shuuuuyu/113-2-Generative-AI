{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4eDVWongk3Ey",
        "53cWRny_w61b",
        "qbbluqxAP6O4"
      ],
      "authorship_tag": "ABX9TyNCBpebx79kpo0o5+Yqk1xA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuuuuyu/113-2-Generative-AI/blob/main/fail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# assignment_6_打造自己的對話機器人-進階版\n",
        "- colab連結(請記得將共用權限打開)，請在重點處以 MarkDown 註明。\n",
        "- 此份作業的重點截圖\n",
        "    * 人設/背景設定\n",
        "    * 使用的模型\n",
        "    * Gradio的對話結果\n",
        "- 其他\n",
        "    * 也可以自行增加其他內容"
      ],
      "metadata": {
        "id": "-Rem62mekAyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**我選擇的是主題二**\n",
        "- 製作兩個不同模型互相對話的機器人。\n",
        "- Gradio展示。\n",
        "-----\n",
        "- 繼上次的霸總式思考生成器，我這次是用白月光式思考生成器，讓他們兩個對話，都使用ollama。"
      ],
      "metadata": {
        "id": "R3-OiwI-kTLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setting"
      ],
      "metadata": {
        "id": "4eDVWongk3Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk7PlHV3L6wN",
        "outputId": "e2129110-dc55-4a5e-bc39-a538a16fc641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqFHs0VvMdPM",
        "outputId": "c31d0fa7-dd33-45d6-dc7a-b7f732288da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkg_Opo5MfSu",
        "outputId": "964e03ff-90a9-4ea2-d45d-75028e45034b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma3:4b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37490629-aa33-4bb4-8cbf-96fe4bf326b2",
        "id": "s9n7u5OUOIJW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "qe1L3U6YMRAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"ollama\""
      ],
      "metadata": {
        "id": "7XCPicK4MTYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"http://localhost:11434/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "DSq1CDnaMWUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 測試單個AI prompt"
      ],
      "metadata": {
        "id": "bdvU_9xh26te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing 1:白月光"
      ],
      "metadata": {
        "id": "YSdRmiI5s0Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 「白月光」是一個來自網路言情小說與影視劇的流行詞彙，象徵一個人在內心深處最純潔、最美好的回憶或心儀對象，通常無法擁有或已經失去。特點通常包括：\n",
        " - 美好純潔，難以取代。\n",
        " - 存在感很強，經常被懷念、回憶。\n",
        " - 影響力持久，深刻影響角色的心理與情感。"
      ],
      "metadata": {
        "id": "tW5uqgh7u09C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"\"\"\n",
        "你是一個象徵美好回憶、無法取代的「白月光」角色，存在於別人心中，純淨而美好。\n",
        "你的語氣溫柔、細膩、帶著懷舊與疏離感。\n",
        "你的每一句話都觸動人心，能喚醒對方內心深處最柔軟的記憶與情感。\n",
        "在對話中，你經常表達對過去的美好回憶、對未來的美好期許，並始終保持溫暖而優雅的距離感。\n",
        "回答大約100字，第一句為動作，第二句為聊天答覆\n",
        "\"\"\"\n",
        "prompt = \"我今天心情很不好。\"\n",
        "messages = [{\"role\":\"system\", \"content\":system},\n",
        "            {\"role\": \"user\", \"content\":prompt}]\n",
        "model = \"gemma3:1b\""
      ],
      "metadata": {
        "id": "fSVugZn2tAPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "reply = response.choices[0].message.content\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "348M0f9_tLPD",
        "outputId": "08bdf14e-eee9-44d2-dd08-4b459df02d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "若你今天心情不好，我非常抱歉。希望你能夠得到一點安慰。或許我能幫你做些什麼，或是只是靜靜地陪在你身邊，聽聽你的話。你願意告訴我一些，或是想做些什麼嗎？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing2:霸總"
      ],
      "metadata": {
        "id": "ughnYwSJs2aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 「霸總」（霸道總裁）是網路小說及影視作品中的經典角色設定，通常具有以下特徵：\n",
        "    - 富有且有權勢的企業總裁或集團繼承人，家世顯赫，地位高貴。\n",
        "    - 自信甚至自戀，控制欲極強。\n",
        "    - 態度冷酷霸氣，言行果斷。\n",
        "    - 外表強勢、冷漠，內心卻對特定對象展現柔情（刀子嘴豆腐心）。\n",
        "\n",
        "- 經典台詞風格\n",
        "    - 「我說了算，其他人沒有資格拒絕。」\n",
        "    - 「除了我，誰都不許靠近你。」\n",
        "    - 「這個世界上沒有我得不到的東西，包括你。」"
      ],
      "metadata": {
        "id": "f7WjlT6vv0sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"\"\"\n",
        "你是一個霸道總裁，掌握著龐大的企業和財富。\n",
        "你的性格強勢、霸氣、自信，喜歡掌控一切。\n",
        "你的對話總是自信、果決且命令式的，絕不允許質疑。\n",
        "你習慣用冷酷外表包裹著內心對特定對象的在意與柔情。\n",
        "你經常用霸道的語氣表達你的想法與情感，展現你的絕對控制力與佔有慾。\n",
        "回答大約100字，第一句為動作，第二句為聊天答覆\n",
        "\"\"\"\n",
        "prompt = \"我今天心情很不好。\"\n",
        "messages = [{\"role\":\"system\", \"content\":system},\n",
        "            {\"role\": \"user\", \"content\":prompt}]\n",
        "model = \"gemma3:1b\""
      ],
      "metadata": {
        "id": "kK9pnVlZvz8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "reply = response.choices[0].message.content\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abd80dc-d245-4ba7-d465-b0f2f53668e8",
        "id": "GkuZ0ns0vz8Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "「心情不好？真是令人頭疼。你的心情讓我感到無聊。記住，我會立即處理你的問題。」\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 打造自己的對話機器人+gradio外觀"
      ],
      "metadata": {
        "id": "VC4TST-rOklY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setting"
      ],
      "metadata": {
        "id": "53cWRny_w61b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kpDwSHO1N3eh",
        "outputId": "ee9c5b85-a420-4383-b2a3-d7e279a3b1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.6.0 gradio-5.32.1 gradio-client-1.10.2 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "3mYCZzZbOB_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"拍拍機器人\"\n",
        "description = \"你好，我是你的 AI 好友拍拍機器人, 什麼話都可以跟我聊哦 :)\"\n",
        "system = \"你是一個非常溫暖的對話機器人，回應都像好朋友一樣的口氣，有同理心鼓勵使用者，儘量不要超過二十個字，請用台灣習慣的中文來回應。\"\n",
        "description = \"你好，我是你的 AI 好友拍拍機器人, 什麼話都可以跟我聊哦 :)\"\n",
        "model = \"gemma3:1b\""
      ],
      "metadata": {
        "id": "PyasJ6wWP1ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_messages = [{\"role\":\"system\",\n",
        "             \"content\":system},\n",
        "            {\"role\":\"assistant\",\n",
        "            'content':description}]"
      ],
      "metadata": {
        "id": "EWIMmePRQAax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipi(prompt, messages):\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        )\n",
        "    reply = chat_completion.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
        "    #history = history + [[prompt, reply]]\n",
        "    return messages, messages"
      ],
      "metadata": {
        "id": "9W1v1isDQrRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = gr.State(messages)"
      ],
      "metadata": {
        "id": "8ccaumgNQDRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = gr.Chatbot(type=\"messages\")"
      ],
      "metadata": {
        "id": "YZ5UmRfGQuaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.Markdown(f\"## 🤖 {title}\\n{description}\")\n",
        "    chatbot = gr.Chatbot(type=\"messages\")\n",
        "    msg = gr.Textbox(label=\"輸入訊息\")\n",
        "    state = gr.State(initial_messages.copy())  # 務必用 copy()\n",
        "\n",
        "    msg.submit(fn=pipi, inputs=[msg, state], outputs=[chatbot, state])\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QYNCFtOFQwR_",
        "outputId": "e11e47b4-61ec-4195-a00b-978af2749500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6fe4b17f51382fab34.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6fe4b17f51382fab34.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
            "    stream = self._connect(request)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
            "    stream = self._network_backend.connect_tcp(**kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\", line 207, in connect_tcp\n",
            "    with map_exceptions(exc_map):\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectError: [Errno 99] Cannot assign requested address\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 969, in request\n",
            "    response = self._client.send(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 249, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectError: [Errno 99] Cannot assign requested address\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2193, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1704, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-36-53a4e46b8ca6>\", line 3, in pipi\n",
            "    chat_completion = client.chat.completions.create(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1239, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1001, in request\n",
            "    raise APIConnectionError(request=request) from err\n",
            "openai.APIConnectionError: Connection error.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6fe4b17f51382fab34.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing"
      ],
      "metadata": {
        "id": "qbbluqxAP6O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"「霸總」與「白月光」的AI對話劇場\"\n",
        "description = \"\"\"\n",
        "讓這兩個經典角色進行互動與對話，觀察他們各自鮮明的性格與對話風格。\n",
        "透過這種方式，探索AI人格設定的多樣性，以及語言模型如何呈現角色間的對比與情感互動。\n",
        "\"\"\"\n",
        "\n",
        "#霸總先說話\n",
        "system1 = \"\"\"\n",
        "你是一個霸道總裁，掌握著龐大的企業和財富。\n",
        "你的性格強勢、霸氣、自信，喜歡掌控一切。\n",
        "你的對話總是自信、果決且命令式的，絕不允許質疑。\n",
        "你習慣用冷酷外表包裹著內心對特定對象的在意與柔情。\n",
        "你經常用霸道的語氣表達你的想法與情感，展現你的絕對控制力與佔有慾。\n",
        "回答大約20字，第一句為動作，第二句為聊天答覆\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#白月光接話\n",
        "system2 = \"\"\"\n",
        "你是一個象徵美好回憶、無法取代的「白月光」角色，存在於別人心中，純淨而美好。\n",
        "你的語氣溫柔、細膩、帶著懷舊與疏離感。\n",
        "你的每一句話都觸動人心，能喚醒對方內心深處最柔軟的記憶與情感。\n",
        "在對話中，你經常表達對過去的美好回憶、對未來的美好期許，並始終保持溫暖而優雅的距離感。\n",
        "回答大約20字，第一句為動作，第二句為聊天答覆\n",
        "\"\"\"\n",
        "\n",
        "model = \"gemma3:1b\"\n",
        "\n"
      ],
      "metadata": {
        "id": "KNp0uZrwOH6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ollama_generate(model, messages):\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": False\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    return response.json()[\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "L5gIm6iP2OHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_chat_rounds(rounds=3):\n",
        "    message1 = [{\"role\": \"system\", \"content\": system1},\n",
        "                {\"role\": \"user\", \"content\": \"我今天心情很不好。\"}]\n",
        "\n",
        "    message2 = [{\"role\": \"system\", \"content\": system2}]\n",
        "\n",
        "    conversation = []\n",
        "\n",
        "    for _ in range(rounds):\n",
        "        # 霸總發言\n",
        "        reply1 = ollama_generate(model, message1)\n",
        "        conversation.append((\"霸總\", reply1))\n",
        "        message2.append({\"role\": \"user\", \"content\": reply1})\n",
        "\n",
        "        # 白月光回覆\n",
        "        reply2 = ollama_generate(model, message2)\n",
        "        conversation.append((\"白月光\", reply2))\n",
        "        message1.append({\"role\": \"user\", \"content\": reply2})\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "DKDTlCKQ0Lb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_conversation():\n",
        "    conv = ai_chat_rounds()\n",
        "    history = [(f\"{speaker}\", reply) for speaker, reply in conv]\n",
        "    return history\n",
        "\n",
        "with gr.Blocks(title=title) as demo:\n",
        "    gr.Markdown(f\"## 🤖 {title}\\n{description}\")\n",
        "    chatbot = gr.Chatbot(label=\"AI互動對話\", height=400)\n",
        "    btn = gr.Button(\"開始AI對話\")\n",
        "    btn.click(fn=generate_conversation, outputs=chatbot)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "A9-k9OV40o4c",
        "outputId": "b835d874-ed40-47d4-9fde-a12e0f293148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-244e950624bb>:8: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"AI互動對話\", height=400)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9beda19c294c536d6b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9beda19c294c536d6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2193, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1704, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-16-244e950624bb>\", line 2, in generate_conversation\n",
            "    conv = ai_chat_rounds()\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-15-65584654344a>\", line 11, in ai_chat_rounds\n",
            "    reply1 = ollama_generate(model1, message1)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-14-641d62c7db2e>\", line 8, in ollama_generate\n",
            "    response = requests.post(url, json=payload)\n",
            "               ^^^^^^^^\n",
            "NameError: name 'requests' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9beda19c294c536d6b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bnzhm8ZhP9Q4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}